{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extraction based Summarizer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4VRWktfcBMjf/YaO2Rn7C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theirfanrahman/Extraction-based-Summarizer/blob/master/Extraction_based_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X3NT9opK0pZ",
        "colab_type": "text"
      },
      "source": [
        "## There are two ways we can do summarize \n",
        "## Extraction and Abstraction\n",
        "## In this notebook we will try Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmBpHrzlL24s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7419bfec-f651-4613-8d6e-bfbe40285a37"
      },
      "source": [
        "#import libraries\n",
        "import nltk\n",
        "#nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import heapq\n",
        "import pandas as pd\n",
        "import re"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImY6kqncNb8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaning sentence by removing unwanted spcial charecter\n",
        "def _clean_article_text(article_text):\n",
        "    \n",
        "    # Removing Square Brackets and Extra Spaces\n",
        "    article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
        "    article_text = re.sub(r'\\s+', ' ', article_text)\n",
        "    \n",
        "    # Removing special characters and digits\n",
        "    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text)\n",
        "    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
        "\n",
        "    return article_text, formatted_article_text;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6Q98Re3NkSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculating count for each word\n",
        "def _create_dictionary_table(formatted_article_text):\n",
        "    \n",
        "    stopword = set(stopwords.words('english'))\n",
        "    word_frequencies = {}\n",
        "    for word in word_tokenize(formatted_article_text):\n",
        "        if word not in stopword:\n",
        "            if word not in word_frequencies.keys():\n",
        "                word_frequencies[word] = 1\n",
        "            else:\n",
        "                word_frequencies[word] += 1\n",
        "                \n",
        "    return word_frequencies;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2GjKnbPN2Cd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculation average score of each word\n",
        "def _calculate_average_score(word_frequencies):\n",
        "\n",
        "    #print(word_frequencies)\n",
        "    maximum_frequncy = max(word_frequencies.values())\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word] = round((word_frequencies[word]/maximum_frequncy),2)\n",
        "\n",
        "    return word_frequencies;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtIZyT5RPXxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculating weight for each sentence\n",
        "def _calculate_sentence_scores(sentence_list, word_frequencies):\n",
        "    \n",
        "    sentence_scores = {}\n",
        "    #sentence_list = str(sentence_list.split(\".\"))\n",
        "    for sent in sentence_list:\n",
        "        sent = sent + \".\"\n",
        "        for word in word_tokenize(sent):\n",
        "            if word in word_frequencies.keys():\n",
        "                # i have considered the sentence have less than 30 word. you can change depending upon your problem\n",
        "                if len(sent.split(' ')) <30:\n",
        "                    if sent not in sentence_scores.keys():\n",
        "                        sentence_scores[sent] = word_frequencies[word]\n",
        "                    else:\n",
        "                        sentence_scores[sent] += word_frequencies[word]\n",
        "\n",
        "    return sentence_scores;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0hpfvLubt4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ffd3e415-2a0a-4074-ecac-f229d2cd84c3"
      },
      "source": [
        "# main function\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # let's define a sentence\n",
        "    sentence = \"\"\"Articles are words that define a noun as specific or unspecific. Consider the following examples: \n",
        "                  After the long day, the cup of tea tasted particularly good. By using the article the, we've shown\n",
        "                  that it was one specific day that was long and one specific cup of tea that tasted good.\"\"\"\n",
        "\n",
        "    # clean text and read \n",
        "    article_text, formatted_article_text = _clean_article_text(sentence)\n",
        "    # split sentence based on dot(.)\n",
        "    sentence_list = article_text.split(\".\")\n",
        "    \n",
        "    word_frequencies = _create_dictionary_table(formatted_article_text)\n",
        "    \n",
        "    word_frequencies = _calculate_average_score(word_frequencies)\n",
        "\n",
        "    sentence_scores = _calculate_sentence_scores(sentence_list, word_frequencies)\n",
        "    # i am considering here top 2 max weighted sentence as i have total 3 sentence in my document.\n",
        "    summary_sentences = heapq.nlargest(2, sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "    summary = ' '.join(summary_sentences)\n",
        "\n",
        "    print(summary)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " By using the article the, we've shown that it was one specific day that was long and one specific cup of tea that tasted good.  Consider the following examples: After the long day, the cup of tea tasted particularly good.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FToRY8-1aR3D",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}